# ç§»é™¤PDFè™•ç†ï¼Œåªä¿ç•™docx/txt/csvçš„ç‰ˆæœ¬
import os
import time
import pickle
import hashlib
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from groq import Groq
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import docx  # ä¸»è¦è™•ç†docxæ–‡ä»¶
import logging
import torch
import faiss
import numpy as np

# ç’°å¢ƒåˆå§‹åŒ–
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¸¸æ•¸è¨­å®š
PROCESSED_DATA_PATH = "processed_data.pkl"
DEFAULT_CSV_PATH = "goat_data.csv"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 160
SIMILARITY_THRESHOLD = 0.3
TOP_K_RESULTS = 20

# é é¢é…ç½®
st.set_page_config(
    page_title="Dr. Goat Pro å°ˆå®¶ç³»çµ±",
    page_icon="ğŸ",
    layout="wide",
    initial_sidebar_state="expanded"
)
st.title("ğŸ Dr. Goat Pro å±±ç¾Šé¤Šæ®–å°ˆå®¶ç³»çµ±")
st.caption("çµåˆæœ€æ–° RAG æŠ€è¡“èˆ‡è¾²æ¥­é ˜åŸŸçŸ¥è­˜çš„æ™ºæ…§è§£æ±ºæ–¹æ¡ˆ")

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def init_system():
    """ç³»çµ±åˆå§‹åŒ–å‡½æ•¸"""
    if not (groq_api_key := os.getenv("GROQ_API_KEY")):
        st.error("GROQ_API_KEY æœªè¨­å®šæ–¼ç’°å¢ƒè®Šæ•¸")
        st.stop()

    try:
        groq_client = Groq(api_key=groq_api_key)
        groq_client.models.list()
    except Exception as e:
        st.error(f"Groq é€£ç·šå¤±æ•—: {str(e)}")
        st.stop()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    embedder = SentenceTransformer(
        'paraphrase-multilingual-MiniLM-L12-v2',
        device=device
    )

    return groq_client, embedder

groq_client, embedder = init_system()

# è³‡æ–™è™•ç†å‡½å¼
def process_docx(file) -> list:
    """è™•ç†Wordæ–‡ä»¶"""
    try:
        doc = docx.Document(file)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])
        return split_text(text)
    except Exception as e:
        logger.error(f"DOCXè™•ç†å¤±æ•—: {str(e)}")
        raise ValueError(f"DOCXè§£æéŒ¯èª¤: {file.name}")

def process_txt(file) -> list:
    """è™•ç†ç´”æ–‡å­—æ–‡ä»¶"""
    try:
        text = file.read().decode("utf-8")
        return split_text(text)
    except Exception as e:
        logger.error(f"TXTè™•ç†å¤±æ•—: {str(e)}")
        raise ValueError(f"TXTè§£æéŒ¯èª¤: {file.name}")

def process_csv(file) -> list:
    """è™•ç†CSVæ–‡ä»¶"""
    try:
        df = pd.read_csv(file)
        if "context" not in df.columns:
            raise ValueError("CSVç¼ºå°‘å¿…è¦æ¬„ä½: context")
        return df["context"].dropna().astype(str).str.strip().tolist()
    except Exception as e:
        logger.error(f"CSVè™•ç†å¤±æ•—: {str(e)}")
        raise ValueError(f"CSVè§£æéŒ¯èª¤: {file.name}")

def split_text(text: str) -> list:
    """æ–‡å­—åˆ†å‰²å‡½æ•¸"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "ã€‚", "ï¼", "ï¼Ÿ", "\n", " "]
    )
    return [chunk.strip() for chunk in splitter.split_text(text) if chunk.strip()]

def get_file_hash(file) -> str:
    """è¨ˆç®—æª”æ¡ˆé›œæ¹Šå€¼"""
    return hashlib.md5(file.getvalue()).hexdigest()

class KnowledgeBase:
    """æ•´åˆ FAISS çš„çŸ¥è­˜åº«ç®¡ç†é¡åˆ¥"""
    def __init__(self):
        self.contexts = []
        self.processed_files = set()
        self.index = None
        self.embedder = embedder

        # è¼‰å…¥é è™•ç†è³‡æ–™
        if os.path.exists(PROCESSED_DATA_PATH):
            self._load_processed_data()
            
        if os.path.exists(DEFAULT_CSV_PATH):
            self._load_default_csv()

    def _load_processed_data(self):
        """è¼‰å…¥è™•ç†éçš„è³‡æ–™"""
        try:
            with open(PROCESSED_DATA_PATH, "rb") as f:
                data = pickle.load(f)
                self.contexts = data["contexts"]
                self.processed_files = data["processed_files"]
                
                if "faiss_index" in data and data["faiss_index"] is not None:
                    self.index = faiss.deserialize_index(data["faiss_index"])
                elif self.contexts:
                    self._build_index()
        except Exception as e:
            logger.error(f"è¼‰å…¥è™•ç†è³‡æ–™å¤±æ•—: {str(e)}")
            self.contexts = []
            self.processed_files = set()

    def _load_default_csv(self):
        """è¼‰å…¥é è¨­CSVæª”æ¡ˆ"""
        try:
            df = pd.read_csv(DEFAULT_CSV_PATH)
            if "context" in df.columns:
                new_data = [c for c in df["context"].dropna().astype(str)
                           if c not in self.contexts]
                if new_data:
                    self.contexts.extend(new_data)
                    self._build_index()
                    logger.info(f"å·²è¼‰å…¥ {len(new_data)} æ¢é è¨­è³‡æ–™")
        except Exception as e:
            logger.error(f"é è¨­CSVè¼‰å…¥å¤±æ•—: {str(e)}")

    def _build_index(self):
        """å»ºç«‹ FAISS ç´¢å¼•"""
        if not self.contexts:
            return
            
        try:
            embeddings = self.embedder.encode(
                self.contexts, 
                convert_to_tensor=False,
                normalize_embeddings=True
            ).astype('float32')
            
            dimension = embeddings.shape[1]
            
            if len(self.contexts) < 10_000:
                self.index = faiss.IndexFlatIP(dimension)
            else:
                nlist = min(100, len(self.contexts) // 10)
                quantizer = faiss.IndexFlatIP(dimension)
                self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
                self.index.train(embeddings)
                
            self.index.add(embeddings)
            logger.info(f"å·²å»ºç«‹ FAISS ç´¢å¼• (é¡å‹: {type(self.index).__name__})")
        except Exception as e:
            logger.error(f"å»ºç«‹ç´¢å¼•å¤±æ•—: {str(e)}")
            self.index = None

    def _save_data(self):
        """ä¿å­˜è³‡æ–™"""
        try:
            data = {
                "contexts": self.contexts,
                "processed_files": self.processed_files,
                "faiss_index": faiss.serialize_index(self.index) if self.index else None
            }
            with open(PROCESSED_DATA_PATH, "wb") as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.error(f"ä¿å­˜è³‡æ–™å¤±æ•—: {str(e)}")

    def add_files(self, uploaded_files):
        """è™•ç†ä¸Šå‚³æª”æ¡ˆ"""
        new_contexts = []
        for file in uploaded_files:
            try:
                file_hash = get_file_hash(file)
                if file_hash in self.processed_files:
                    continue

                if file.name.endswith(".docx"):
                    processed_data = process_docx(file)
                elif file.name.endswith(".txt"):
                    processed_data = process_txt(file)
                elif file.name.endswith(".csv"):
                    processed_data = process_csv(file)
                else:
                    st.warning(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file.name}")
                    continue

                logger.info(f"æª”æ¡ˆ {file.name} è§£æå‡º {len(processed_data)} æ¢è³‡æ–™")
                new_contexts.extend(processed_data)
                self.processed_files.add(file_hash)

            except Exception as e:
                st.error(str(e))
                continue

        if new_contexts:
            unique_contexts = list(set(new_contexts) - set(self.contexts))
            if unique_contexts:
                self.contexts.extend(unique_contexts)
                self._build_index()
                self._save_data()
                logger.info(f"æ–°å¢ {len(unique_contexts)} æ¢è³‡æ–™åˆ°çŸ¥è­˜åº«")
                return len(unique_contexts)
        return 0

    def semantic_search(self, query: str) -> list:
        """èªç¾©æœç´¢"""
        if not self.contexts or not self.index:
            logger.warning("çŸ¥è­˜åº«ç‚ºç©ºæˆ–ç´¢å¼•æœªå»ºç«‹")
            return []

        try:
            query_embed = self.embedder.encode(
                query,
                convert_to_tensor=False,
                normalize_embeddings=True
            ).astype('float32').reshape(1, -1)

            scores, indices = self.index.search(query_embed, TOP_K_RESULTS)

            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx >= 0 and score >= SIMILARITY_THRESHOLD:
                    results.append({
                        "score": float(score),
                        "text": self.contexts[idx]
                    })

            logger.info(f"æ‰¾åˆ° {len(results)} æ¢ç›¸é—œçµæœ (æœ€é«˜åˆ†: {max([r['score'] for r in results], default=0):.2f})")
            return sorted(results, key=lambda x: x["score"], reverse=True)
        except Exception as e:
            logger.error(f"æœç´¢å¤±æ•—: {str(e)}")
            return []

class ChatEngine:
    """å•ç­”å¼•æ“"""
    def __init__(self, groq_client):
        self.client = groq_client
        self.retry_config = {
            "retries": 3,
            "wait_time": 45,
            "timeout": 20
        }

    def generate_response(self, query: str, contexts: list) -> str:
        if not contexts:
            return "âš ï¸ ç›®å‰çŸ¥è­˜åº«ä¸­æ²’æœ‰ç›¸é—œè³‡æ–™ï¼Œè«‹å…ˆä¸Šå‚³æŠ€è¡“æ–‡ä»¶ã€‚"

        combined_context = "\n\n".join([c["text"] for c in contexts])
        prompt = f"""ä½ æ˜¯ä¸€ä½å±±ç¾Šé¤Šæ®–å°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™ç”¨ä¸­æ–‡å›ç­”å•é¡Œï¼š

        **å•é¡Œ**ï¼š{query}

        **åƒè€ƒè³‡æ–™**ï¼š
        {combined_context[:3000]}

        å›ç­”æ™‚è«‹éµå®ˆï¼š
        1. å°ˆæ¥­ä½†æ˜“æ‡‚ï¼Œä½¿ç”¨å°ç£å¸¸ç”¨è¡“èª
        2. é‡è¦æ•¸æ“šéœ€æ˜ç¢ºæ¨™ç¤º
        3. è‹¥è³‡æ–™ä¸è¶³è«‹èªªæ˜éœ€è¦è£œå……çš„è³‡è¨Š
        4. åˆ†é»æ¢åˆ—èªªæ˜ï¼ˆå¦‚æœ‰å¤šé …è¦é»ï¼‰
        """

        for attempt in range(self.retry_config["retries"]):
            try:
                response = self.client.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    model="llama-3.3-70b-versatile",
                    temperature=0.3,
                    max_tokens=1024,
                    top_p=0.9
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if "rate_limit" in str(e) and attempt < self.retry_config["retries"] - 1:
                    time.sleep(self.retry_config["wait_time"])
                else:
                    logger.error(f"APIè«‹æ±‚å¤±æ•—: {str(e)}")
                    return f"âŒ æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦ï¼ˆéŒ¯èª¤ä»£ç¢¼ï¼š{str(e)[:50]}ï¼‰"

def init_ui():
    """åˆå§‹åŒ–ä½¿ç”¨è€…ä»‹é¢"""
    if "kb" not in st.session_state:
        st.session_state.kb = KnowledgeBase()

    if "chat_engine" not in st.session_state:
        st.session_state.chat_engine = ChatEngine(groq_client)

    if "messages" not in st.session_state:
        st.session_state.messages = []

def main():
    init_ui()

    with st.sidebar:
        st.header("ğŸ“ çŸ¥è­˜åº«ç®¡ç†")
        
        uploaded_files = st.file_uploader(
            "ä¸Šå‚³æŠ€è¡“æ–‡ä»¶ï¼ˆDOCX/TXT/CSVï¼‰",
            type=["docx", "txt", "csv"],
            accept_multiple_files=True
        )
        if uploaded_files:
            with st.spinner(f"æ­£åœ¨è™•ç† {len(uploaded_files)} å€‹æª”æ¡ˆ..."):
                added_count = st.session_state.kb.add_files(uploaded_files)
                if added_count > 0:
                    st.success(f"æ–°å¢ {added_count} æ¢æŠ€è¡“è³‡æ–™ï¼")
                    if any(file.name.endswith('.csv') for file in uploaded_files):
                        total_csv_data = sum(len(pd.read_csv(file)) for file in uploaded_files if file.name.endswith('.csv'))
                        st.markdown(f"ğŸ”¢ CSVæª”æ¡ˆç¸½å…±åŒ…å« {total_csv_data} æ¢è³‡æ–™ã€‚")
                    st.rerun()

        st.divider()
        st.markdown(f"**ç•¶å‰çŸ¥è­˜åº«çµ±è¨ˆ** ğŸ“Š")
        st.markdown(f"- ç¸½è³‡æ–™æ¢æ•¸ï¼š{len(st.session_state.kb.contexts)}")
        st.markdown(f"- å·²è™•ç†æª”æ¡ˆæ•¸ï¼š{len(st.session_state.kb.processed_files)}")
        st.markdown(f"- ç´¢å¼•é¡å‹ï¼š{type(st.session_state.kb.index).__name__ if st.session_state.kb.index else 'ç„¡'}")

        if st.button("ğŸ“‚ é¡¯ç¤ºæ‰€æœ‰çŸ¥è­˜åº«å…§å®¹"):
            if st.session_state.kb.contexts:
                st.markdown("### çŸ¥è­˜åº«æ‰€æœ‰å…§å®¹")
                for i, context in enumerate(st.session_state.kb.contexts, 1):
                    st.markdown(f"**å…§å®¹ {i}**")
                    st.markdown(f"```\n{context}\n```")
                    st.markdown("---")
            else:
                st.warning("çŸ¥è­˜åº«ä¸­ç›®å‰æ²’æœ‰å…§å®¹ã€‚")

        st.divider()
        if st.button("ğŸ”„ æ¸…é™¤å°è©±è¨˜éŒ„"):
            st.session_state.messages = []
            st.rerun()

        if st.button("âš ï¸ é‡ç½®çŸ¥è­˜åº«"):
            st.session_state.kb = KnowledgeBase()
            st.rerun()

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if query := st.chat_input("è¼¸å…¥é—œæ–¼å±±ç¾Šé¤Šæ®–çš„æŠ€è¡“å•é¡Œ..."):
        st.session_state.messages.append({"role": "user", "content": query})
        st.chat_message("user").write(query)

        with st.spinner("ğŸ” æ­£åœ¨æœç´¢æŠ€è¡“è³‡æ–™åº«..."):
            search_results = st.session_state.kb.semantic_search(query)

        if not search_results:
            response = "âš ï¸ æœªæ‰¾åˆ°ç›¸é—œæŠ€è¡“è³‡æ–™ï¼Œå»ºè­°è£œå……ä»¥ä¸‹è³‡è¨Šï¼š\n- ç–¾ç—…ç—‡ç‹€æè¿°\n- é£¼é¤Šç’°å¢ƒæ¢ä»¶\n- æª¢æ¸¬å ±å‘Šæ•¸æ“š"
        else:
            with st.expander("ğŸ“„ æª¢ç´¢åˆ°çš„æŠ€è¡“è³‡æ–™ç‰‡æ®µ", expanded=True):
                for i, res in enumerate(search_results, 1):
                    st.markdown(f"**åŒ¹é…åº¦ {res['score']:.2%} (ç‰‡æ®µ {i})**")
                    st.markdown(f"```\n{res['text']}\n```")
                    st.markdown("---")

            with st.spinner("ğŸ§  æ­£åœ¨ç”Ÿæˆå°ˆå®¶å»ºè­°..."):
                response = st.session_state.chat_engine.generate_response(query, search_results)

        st.session_state.messages.append({"role": "assistant", "content": response})
        st.chat_message("assistant").write(response)

if __name__ == "__main__":
    main()